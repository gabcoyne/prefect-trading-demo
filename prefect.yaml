# Prefect deployment configuration for Trading Partition Demo
# Deploy to Prefect Cloud using: prefect deploy --all
#
# This will automatically:
# 1. Build Docker image for linux/amd64
# 2. Push to ECR registry
# 3. Register both parent and child deployments

# Define project-level settings
name: Trading Partition Demo
prefect-version: 3.0.0

# Build steps - build and push Docker image automatically
build:
  - prefect_docker.deployments.steps.build_docker_image:
      id: build-image
      requires: prefect-docker>=0.5.0
      image_name: "455346737763.dkr.ecr.us-east-2.amazonaws.com/se-demos/trading-partition-demo"
      tag: "latest"
      platform: "linux/amd64"
      dockerfile: "Dockerfile"
      nocache: true
  - prefect.deployments.steps.run_shell_script:
      id: ecr-login
      script: sh -c "aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 455346737763.dkr.ecr.us-east-2.amazonaws.com"
      stream_output: true
  - prefect_docker.deployments.steps.push_docker_image:
      requires: prefect-docker>=0.5.0
      image_name: "{{ build-image.image }}"
      tag: "{{ build-image.tag }}"

# Pull steps - used by workers to pull the image
pull:
  - prefect.deployments.steps.set_working_directory:
      directory: /app

# Deployment definitions
deployments:
  # 1. Data Ingestion - Fetch live market data from Yahoo Finance
  - name: ingest-market-data
    version: null
    tags: ["ingestion", "data-pipeline"]
    description: "Fetch live stock, VIX, and SPX data from Yahoo Finance API"
    entrypoint: flows/ingest_market_data_flow.py:ingest_market_data
    parameters:
      symbols:
        [
          "AAPL",
          "MSFT",
          "GOOGL",
          "AMZN",
          "NVDA",
          "META",
          "TSLA",
          "BRK-B",
          "V",
          "UNH",
        ]
      interval: "1h"
      output_stocks_path: "s3://se-demo-raw-data-files/spx_holdings_hourly.parquet"
      output_vix_path: "s3://se-demo-raw-data-files/vix_hourly.parquet"
      output_spx_path: "s3://se-demo-raw-data-files/spx_hourly.parquet"
    work_pool:
      name: demo_eks
      work_queue_name: null
      job_variables:
        image: "{{ build-image.image }}"
        image_pull_policy: Always
    schedules: []

  # 2. Data Validation - Quality checks before processing
  - name: validate-data
    version: null
    tags: ["validation", "data-quality"]
    description: "Validate data quality: missing values, outliers, market data availability"
    entrypoint: flows/validate_data_flow.py:validate_data
    parameters:
      parquet_path: "s3://se-demo-raw-data-files/spx_holdings_hourly.parquet"
      vix_path: "s3://se-demo-raw-data-files/vix_hourly.parquet"
      spx_path: "s3://se-demo-raw-data-files/spx_hourly.parquet"
    work_pool:
      name: demo_eks
      work_queue_name: null
      job_variables:
        image: "{{ build-image.image }}"
        image_pull_policy: Always
    schedules: []

  # 3. Main Orchestrator - Top-level coordinator
  - name: orchestrator
    version: null
    tags: ["orchestrator", "parent"]
    description: "Orchestrates trading pipeline: triggers parallel symbol analysis flows"
    entrypoint: flows/orchestrator_flow.py:trading_orchestrator
    parameters:
      num_contracts: 10
    work_pool:
      name: demo_eks
      work_queue_name: null
      job_variables:
        image: "{{ build-image.image }}"
        image_pull_policy: Always
    schedules: []

  # 4. Symbol Flow - Per-contract analysis with market context
  - name: analyze-symbol
    version: null
    tags: ["symbol", "analysis", "market-context"]
    description: "Analyze single contract across all timestamps with VIX/SPX/beta enrichment"
    entrypoint: flows/analyze_symbol_flow.py:analyze_symbol
    parameters:
      contract: "AAPL"
    work_pool:
      name: demo_eks
      work_queue_name: null
      job_variables:
        image: "{{ build-image.image }}"
        image_pull_policy: Always
    schedules: []

  # 5. Portfolio Aggregation - Final analytics across all results
  - name: aggregate-portfolio
    version: null
    tags: ["aggregation", "portfolio", "analytics"]
    description: "Aggregate results and calculate portfolio-level metrics (P&L, Sharpe, win rate)"
    entrypoint: flows/aggregate_portfolio_flow.py:aggregate_portfolio
    parameters:
      output_dir: "s3://se-demo-raw-data-files/trading-results"
    work_pool:
      name: demo_eks
      work_queue_name: null
      job_variables:
        image: "{{ build-image.image }}"
        image_pull_policy: Always
    schedules: []
